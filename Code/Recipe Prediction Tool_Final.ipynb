{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8490b1ca",
   "metadata": {},
   "source": [
    "# Group 3 - Kitchen Assistant : A Recipe Prediction Tool\n",
    "\n",
    "### About: Our tool is designed to provide suggested recipes based on a user's input of available ingredients\n",
    "\n",
    "### User Directions: Provide recipe ingredients, seperated by commas, recieve five recipe suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49aae8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\18502\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\18502\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "078d3443",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. READ DATA\n",
    "\n",
    "Data source: Food Ingredients and Recipe Dataset.csv\n",
    "\n",
    "Columns:\n",
    "    Title\n",
    "    Ingredients\n",
    "    Instructions\n",
    "    Image_Name\n",
    "    Cleaned_Ingredients\n",
    "    \n",
    "Columns Used:\n",
    "    Title\n",
    "    Ingredients\n",
    "    \n",
    "Pupose: Read data, remove unnecessary columns\n",
    "\"\"\"\n",
    "\n",
    "# Import Data\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "\n",
    "def read_data(file):\n",
    "    # Read in data, keep only title and ingredients columns\n",
    "    data = pd.read_csv(file)\n",
    "    data_subset = data[[\"Title\", \"Ingredients\"]]\n",
    "    \n",
    "    return data_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc0c8e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TOKENIZE DATA\n",
    "Purpose: Tokenize data, create column to maintain original data for comparison\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize sentences and words. Print exmaple for testing\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "def tokenize_recipes(dataframe):\n",
    "    # Create empty column for tokenized ingredients\n",
    "    dataframe['Tokenized Ingredients'] = np.nan\n",
    "\n",
    "    for i in range(len(dataframe)):\n",
    "        dataframe['Ingredients'][i].lower() # lower case everything\n",
    "        dataframe['Tokenized Ingredients'][i] = word_tokenize(dataframe['Ingredients'][i])\n",
    "     \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55e1e67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "REMOVE PUNCTUATION\n",
    "Purpose: Remove punctuation, create column to maintain original data for comparison\n",
    "\"\"\"\n",
    "#Remove punctuations. Only print the first 20 words.\n",
    "import string\n",
    "\n",
    "def remove_punct(text):\n",
    "    return \"\".join([c for c in text if c not in string.punctuation])\n",
    "\n",
    "def remove_pucntuation_from_recipes(dataframe):\n",
    "    # Empty column for ingredients without punctuation\n",
    "    dataframe['Remove Punct'] = np.nan\n",
    "\n",
    "    for i in range(len(dataframe)):\n",
    "        dataframe['Remove Punct'][i] = [remove_punct(w.lower()) for w in dataframe['Tokenized Ingredients'][i] if remove_punct(w)!='']\n",
    "        \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c450ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "REMOVE STOPWORDS\n",
    "Purpose: Remove stopwords, create column to maintain original data for comparison\n",
    "\"\"\"\n",
    "#Remove stop words and count the distinct cleaned words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "def remove_stopwords(dataframe):\n",
    "    # Empty column for ingredients without stop words\n",
    "    dataframe['Stop Words Removed'] = np.nan\n",
    "\n",
    "    for i in range(len(dataframe)):\n",
    "        dataframe['Stop Words Removed'][i] = [word for word in dataframe['Remove Punct'][i] if word not in stop_words]\n",
    "        \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ada20ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LEMMATIZE INGREDIENTS\n",
    "Purpose: Lemmatize ingredients, create column to maintain original data for comparison\n",
    "\"\"\"\n",
    "#Lemmatize the cleaned words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_data(dataframe):\n",
    "    #Empty column for lemmatized ingredients\n",
    "    dataframe['Lemmatized'] = np.nan\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for i in range(len(dataframe)):\n",
    "        dataframe['Lemmatized'][i] = [lemmatizer.lemmatize(word) for word in dataframe['Stop Words Removed'][i]]\n",
    "        \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b2e334e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "REMOVE NUMERICal STRINGS\n",
    "Purpose: Remove numerical strings caused by ingredient measurements\n",
    "Source: Remove strings from a list that contains numbers in python. (n.d.). Stack Overflow. https://stackoverflow.com/questions/16084642/remove-strings-from-a-list-that-contains-numbers-in-python\n",
    "\"\"\"\n",
    "\n",
    "def remove_numerics(dataframe):\n",
    "    #Empty column for data without numbers or strings with numbers\n",
    "    dataframe['Remove Numerics'] = np.nan\n",
    "\n",
    "    for i in range(len(dataframe)):\n",
    "        dataframe['Remove Numerics'][i] = [item for item in dataframe['Lemmatized'][i] if item.isalpha()]\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2f474bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "POS TAGGING\n",
    "Remove excess columns up to this point\n",
    "\n",
    "Purpose: Remove excess columns created up to this point, use pos-tagging to only maintain noun type words\n",
    "Source: How to use pos_tag in NLTK? (n.d.). Stack Overflow. https://stackoverflow.com/questions/47519987/how-to-use-pos-tag-in-nltk\n",
    "\"\"\"\n",
    "\n",
    "from nltk import pos_tag\n",
    "    \n",
    "def pos_tagging(dataframe):\n",
    "# Remove Excess Columns now that we are almost done with preprocessing\n",
    "    recipe_data = dataframe[[\"Title\", \"Ingredients\", \"Remove Numerics\"]]\n",
    "\n",
    "    # Empty column for new data\n",
    "    recipe_data['Ingredient Nouns'] = np.nan\n",
    "\n",
    "    for i in range(len(recipe_data)):\n",
    "        \n",
    "        tags = pos_tag(recipe_data['Remove Numerics'][i])\n",
    "        nouns = [word for word,pos in tags if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS')]\n",
    "\n",
    "        recipe_data['Ingredient Nouns'][i] = nouns    \n",
    "    \n",
    "    return recipe_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21a08800",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CONVERT LIST OF WORDS BACK INTO STRINGS OF RECIPE INGREDIENTS\n",
    "Purpose: Ingredients exist as list of words, convert each row back to a string of all ingredients\n",
    "\"\"\"\n",
    "\n",
    "def convert_to_strings(dataframe):\n",
    "    # Join the lists of ingredients for comparison\n",
    "    # Empty column for ingredients data input (string instead of list)\n",
    "    dataframe['Ingredient Input'] = np.nan\n",
    "\n",
    "    for i in range(len(dataframe)):\n",
    "        dataframe['Ingredient Input'][i] = \" \".join(dataframe['Ingredient Nouns'][i])\n",
    "\n",
    "    data_subset = dataframe[[\"Title\", \"Ingredients\", \"Ingredient Input\"]] # create final dataframe\n",
    "    \n",
    "    return data_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57e82a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "COSINE SIMILARITY FUNCTIONS\n",
    "Purpose: \n",
    "\"\"\"\n",
    "# Calculate cosine similarity given 2 sentence strings. (n.d.). Stack Overflow. https://stackoverflow.com/questions/15173225/calculate-cosine-similarity-given-2-sentence-strings\n",
    "\n",
    "import math\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "WORD = re.compile(r\"\\w+\")\n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "    sum1 = sum([vec1[x] ** 2 for x in list(vec1.keys())])\n",
    "    sum2 = sum([vec2[x] ** 2 for x in list(vec2.keys())])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text):\n",
    "    words = WORD.findall(text)\n",
    "    \n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a87e074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter ingredients, seperated by commas:strawberry, rice, tomato\n",
      "Your suggested recipes are:  Parboiled Rice , Yogurt Granita , Paella with Tomatoes and Eggs , Sweet and Spicy Chicken Drumsticks , and  Stir-Fried Egg and Tomato\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    user_ingredients = input(\"Enter ingredients, seperated by commas:\")\n",
    "    ingredients = user_ingredients\n",
    "    \n",
    "    file = 'Food Ingredients and Recipe Dataset.csv'\n",
    "    \n",
    "    # Data Preprocessing\n",
    "    data = read_data(file)\n",
    "    tokenized_recipes = tokenize_recipes(data)\n",
    "    punct_removed = remove_pucntuation_from_recipes(tokenized_recipes)\n",
    "    remove_stop = remove_stopwords(punct_removed)\n",
    "    lemma_recipes = lemmatize_data(remove_stop)\n",
    "    remove_num = remove_numerics(lemma_recipes)\n",
    "    tagged = pos_tagging(remove_num)\n",
    "    final_data = convert_to_strings(tagged)\n",
    "    \n",
    "    # METHOD FOR TESTING: Create list to hold all similarity scores, take the first record and compare to all other records.\n",
    "    # Iterate over the entire dataframe to get a similarity score for how the first record compares to each subsequent record\n",
    "    # Save each score for later use\n",
    "    \n",
    "    # UPDATE FOR REAL USE: Use user input as string to compare to corpus. Convert both user input and corpus into vectors to\n",
    "    # then compute cosine similarity. Save all similarity scores. Sort scores, print top 5 recipes\n",
    "\n",
    "    sim_scores = []\n",
    "\n",
    "    test_recipe = ingredients # assign user input \n",
    "    test_vector = text_to_vector(test_recipe) # vectorize user input\n",
    "\n",
    "    for i in range(len(final_data)):\n",
    "        compare_recipe = final_data['Ingredient Input'][i] # iterate through corpus to compare all recipes to user input\n",
    "        compare_vector = text_to_vector(compare_recipe)\n",
    "\n",
    "        cosine = get_cosine(test_vector, compare_vector)\n",
    "\n",
    "        sim_scores.append(cosine)\n",
    "\n",
    "    list_of_scores_with_index = []\n",
    "    list_of_scores_with_recipe = []\n",
    "    list_of_recipes = final_data['Title'].to_list()\n",
    "\n",
    "    for i in range(len(sim_scores)):\n",
    "        list_of_scores_with_index.append([sim_scores[i], i])\n",
    "\n",
    "    # Create list to hold title of recipe and similarity scores\n",
    "    for i in range(len(list_of_scores_with_index)):\n",
    "        list_of_scores_with_recipe.append([list_of_scores_with_index[i][0],list_of_recipes[list_of_scores_with_index[i][1]]])\n",
    "\n",
    "    # Sort cosine similarity scores\n",
    "    list_of_scores_descending = sorted(list_of_scores_with_recipe, key = lambda x : x[0], reverse=True)\n",
    "\n",
    "    # Print top five recipes\n",
    "    print('Your suggested recipes are: ' ,list_of_scores_descending[0][1],',',\n",
    "          list_of_scores_descending[1][1],',',list_of_scores_descending[2][1],',',\n",
    "          list_of_scores_descending[3][1],', and ',list_of_scores_descending[4][1])\n",
    "    \n",
    "    recipes = [list_of_scores_descending[0][1], list_of_scores_descending[1][1],list_of_scores_descending[2][1],\n",
    "              list_of_scores_descending[3][1],list_of_scores_descending[4][1]]\n",
    "    recipe_output = data[data['Title'].isin(recipes)]\n",
    "    \n",
    "    output = recipe_output[['Title', 'Ingredients']]\n",
    "    \n",
    "    output.to_excel('recipe_recommendations.xlsx', index=False) # export recipes with title and ingredients\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cb81e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
